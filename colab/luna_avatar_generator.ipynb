{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Luna Avatar Generator - Flux Kontext Image-to-Image Pipeline\n\nGenerates **4,870** Luna avatar variations using **Flux.1 Kontext Dev** on a free Colab T4 GPU.\n\n**Prerequisites:**\n1. A [Hugging Face account](https://huggingface.co/join) with access to [FLUX.1-Kontext-dev](https://huggingface.co/black-forest-labs/FLUX.1-Kontext-dev) (accept the license on the model page)\n2. An HF access token stored in **Colab Secrets** as `HF_TOKEN` (Settings gear icon > Secrets > Add `HF_TOKEN`)\n3. A `BASE_IMAGES/` folder uploaded to your Google Drive root with 23 reference images\n4. `prompt_manifest.py` uploaded to your Drive root (or `Drive/colab/` folder)\n\n**How it works:** Takes 23 base reference images of Luna (9 regular outfits + 14 costumes),\nthen applies pose/emotion prompts via image-to-image editing. Regular outfit prompts are\nmultiplied by 4 hairstyle variants (hair down, messy bun, ponytail, braid). Costume prompts\nare thematic and not multiplied.\n\n**Priority order:** `dress.jpg` images generate first (396 images), then other outfits, then costumes.\n\n**Parallel mode:** Set `CHUNK_INDEX` and `TOTAL_CHUNKS` in cell 4 to split across multiple Colab instances.\n\n**Resume-safe** - skips images that already exist on Drive. Just re-run after timeout.\n\nResults are saved to Google Drive at `/MyDrive/luna_avatars/` with YAML config output."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 1. Setup Environment & Install Dependencies\n#@markdown Installs Flux Kontext pipeline and quantization libraries. ~3-4 min on first run.\n#@markdown\n#@markdown **IMPORTANT:** You must have a Hugging Face account and accept the\n#@markdown [FLUX.1-Kontext-dev license](https://huggingface.co/black-forest-labs/FLUX.1-Kontext-dev)\n#@markdown before running this notebook. Store your HF token in Colab Secrets as `HF_TOKEN`.\n\nimport subprocess\nimport sys\nimport os\n\n# Install diffusers from main (Kontext support) + quantization deps\nsubprocess.check_call([\n    sys.executable, '-m', 'pip', 'install', '-q',\n    'git+https://github.com/huggingface/diffusers.git',\n    'transformers', 'accelerate', 'sentencepiece', 'protobuf',\n    'safetensors', 'Pillow', 'pyyaml', 'huggingface_hub',\n    'optimum-quanto',\n])\n\n# Authenticate with Hugging Face (FLUX.1-Kontext-dev is a gated model)\nfrom huggingface_hub import login\ntry:\n    from google.colab import userdata\n    hf_token = userdata.get('HF_TOKEN')\n    login(token=hf_token)\n    print('Authenticated with HF token from Colab Secrets.')\nexcept Exception:\n    # Fall back to interactive login if secret not found\n    print('HF_TOKEN not found in Colab Secrets. Trying interactive login...')\n    login()\n\n# Verify GPU\nimport torch\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n    print(f'GPU: {gpu_name} ({gpu_mem:.1f} GB)')\n    print(f'CUDA: {torch.version.cuda}')\n    print(f'PyTorch: {torch.__version__}')\nelse:\n    raise RuntimeError(\n        'No GPU detected! Go to Runtime > Change runtime type > T4 GPU'\n    )\n\nprint('\\nEnvironment ready.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 2. Mount Google Drive & Upload Base Images\n#@markdown Mounts Drive and copies base images to the working directory.\n#@markdown **IMPORTANT:** Upload the `BASE_IMAGES/` folder to your Google Drive root first!\n#@markdown Also upload `prompt_manifest.py` to your Drive root (or the `colab/` folder).\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nOUTPUT_ROOT = '/content/drive/MyDrive/luna_avatars'\nBASE_IMAGE_DIR = '/content/drive/MyDrive/BASE_IMAGES'\nLOCAL_BASE_DIR = '/content/base_images'\n\nos.makedirs(OUTPUT_ROOT, exist_ok=True)\nos.makedirs(LOCAL_BASE_DIR, exist_ok=True)\n\n# Copy base images to local storage for faster access (supports jpg, png, webp)\nimport shutil\nif os.path.isdir(BASE_IMAGE_DIR):\n    for f in os.listdir(BASE_IMAGE_DIR):\n        if f.lower().endswith(('.jpg', '.jpeg', '.png', '.webp')):\n            src = os.path.join(BASE_IMAGE_DIR, f)\n            dst = os.path.join(LOCAL_BASE_DIR, f)\n            shutil.copy2(src, dst)\n            print(f'  Copied: {f}')\n    print(f'\\n{len(os.listdir(LOCAL_BASE_DIR))} base images ready.')\nelse:\n    print(f'WARNING: {BASE_IMAGE_DIR} not found on Drive!')\n    print('Please upload your BASE_IMAGES folder to Google Drive root.')\n    print('Expected: 9 regular outfit images + 14 costume images (23 total)')\n\n# Copy prompt_manifest.py to working directory for import\nMANIFEST_LOCATIONS = [\n    '/content/drive/MyDrive/prompt_manifest.py',\n    '/content/drive/MyDrive/colab/prompt_manifest.py',\n    '/content/drive/MyDrive/Luna/prompt_manifest.py',\n    '/content/drive/MyDrive/Luna/colab/prompt_manifest.py',\n]\nmanifest_found = False\nfor loc in MANIFEST_LOCATIONS:\n    if os.path.exists(loc):\n        shutil.copy2(loc, '/content/prompt_manifest.py')\n        print(f'\\nCopied prompt_manifest.py from {loc}')\n        manifest_found = True\n        break\nif not manifest_found:\n    print('\\nWARNING: prompt_manifest.py not found on Drive!')\n    print('Upload it to your Drive root or Drive/colab/ folder.')\n\n# Progress tracking\nPROGRESS_FILE = os.path.join(OUTPUT_ROOT, '_progress.txt')\nSTATUS_FILE = os.path.join(OUTPUT_ROOT, '_status.txt')\n\ndef update_status(msg):\n    with open(STATUS_FILE, 'w') as f:\n        f.write(msg)\n    print(msg)\n\ndef log_progress(outfit, filename, idx, total):\n    with open(PROGRESS_FILE, 'a') as f:\n        f.write(f'{idx}/{total} | {outfit}/{filename}\\n')\n\nupdate_status('MOUNTED')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 3. Load Flux.1 Kontext Dev (Quantized for T4)\n#@markdown Loads Flux Kontext with FP8 quantization via optimum-quanto.\n#@markdown This fits on the free T4 GPU (15 GB VRAM).\n\nimport torch\nimport gc\nfrom diffusers import FluxKontextPipeline, FluxTransformer2DModel\nfrom transformers import T5EncoderModel\nfrom optimum.quanto import freeze, qfloat8, quantize\n\nupdate_status('LOADING_MODEL')\n\nMODEL_ID = 'black-forest-labs/FLUX.1-Kontext-dev'\nDTYPE = torch.bfloat16\n\n# Load and quantize transformer (largest component)\nprint('Loading transformer...')\ntransformer = FluxTransformer2DModel.from_pretrained(\n    MODEL_ID, subfolder='transformer', torch_dtype=DTYPE\n)\nprint('Quantizing transformer to FP8...')\nquantize(transformer, weights=qfloat8)\nfreeze(transformer)\n\n# Load and quantize T5 text encoder\nprint('Loading T5 text encoder...')\ntext_encoder_2 = T5EncoderModel.from_pretrained(\n    MODEL_ID, subfolder='text_encoder_2', torch_dtype=DTYPE\n)\nprint('Quantizing T5 to FP8...')\nquantize(text_encoder_2, weights=qfloat8)\nfreeze(text_encoder_2)\n\n# Build the pipeline with pre-quantized components (avoids double-loading into VRAM)\nprint('Building pipeline...')\npipe = FluxKontextPipeline.from_pretrained(\n    MODEL_ID,\n    transformer=transformer,\n    text_encoder_2=text_encoder_2,\n    torch_dtype=DTYPE,\n)\npipe.enable_model_cpu_offload()\n\n# Free loading overhead\ngc.collect()\ntorch.cuda.empty_cache()\n\nvram_used = torch.cuda.memory_allocated() / (1024**3)\nprint(f'\\nPipeline ready. VRAM: {vram_used:.2f} GB')\nupdate_status('MODEL_READY')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4. Load Base Images & Import Manifest\n#@markdown Loads all 23 base reference images and imports the prompt manifest.\n#@markdown\n#@markdown **Parallel mode:** To split across multiple Colab instances, set\n#@markdown `TOTAL_CHUNKS` to the number of instances and `CHUNK_INDEX` to this\n#@markdown instance's index (0-based). Each instance gets a different slice.\n#@markdown Leave both at 0 for single-instance mode (generates everything).\n\nfrom PIL import Image\nimport sys\n\n# â”€â”€ PARALLEL CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Set these to split work across multiple Colab instances:\n#   CHUNK_INDEX = 0, TOTAL_CHUNKS = 0  -> single instance, all 4870 images\n#   CHUNK_INDEX = 0, TOTAL_CHUNKS = 3  -> instance 1 of 3 (~1600 images)\n#   CHUNK_INDEX = 1, TOTAL_CHUNKS = 3  -> instance 2 of 3 (~1600 images)\n#   CHUNK_INDEX = 2, TOTAL_CHUNKS = 3  -> instance 3 of 3 (~1600 images)\nCHUNK_INDEX = 0   #@param {type:\"integer\"}\nTOTAL_CHUNKS = 0  #@param {type:\"integer\"}\n\n# Import prompt_manifest (copied to /content/ in cell 2)\nsys.path.insert(0, '/content')\nfrom prompt_manifest import BASE_IMAGES, MANIFEST as FULL_MANIFEST, MASTER_PROMPTS, HAIRSTYLE_VARIANTS, get_chunk\n\n# Apply chunking if configured\nif TOTAL_CHUNKS > 1:\n    MANIFEST = get_chunk(CHUNK_INDEX, TOTAL_CHUNKS)\n    print(f'PARALLEL MODE: Chunk {CHUNK_INDEX + 1} of {TOTAL_CHUNKS}')\n    print(f'This instance: {len(MANIFEST)} images (of {len(FULL_MANIFEST)} total)')\nelse:\n    MANIFEST = FULL_MANIFEST\n    print(f'SINGLE MODE: All {len(MANIFEST)} images')\n\n# Show priority: first base_key in this chunk's manifest\nfrom collections import Counter\nbase_order = []\nfor m in MANIFEST:\n    if m['base_key'] not in base_order:\n        base_order.append(m['base_key'])\nbase_counts = Counter(m['base_key'] for m in MANIFEST)\nprint(f'\\nGeneration order for this {\"chunk\" if TOTAL_CHUNKS > 1 else \"run\"}:')\ncumulative = 0\nfor key in base_order[:5]:\n    count = base_counts[key]\n    cumulative += count\n    print(f'  {key}: {count} images (cumulative: {cumulative})')\nif len(base_order) > 5:\n    print(f'  ... and {len(base_order) - 5} more base images')\n\n# Load only the base images needed for this chunk\nneeded_bases = set(m['base_key'] for m in MANIFEST)\nloaded_bases = {}\nfor key, info in BASE_IMAGES.items():\n    if key not in needed_bases:\n        continue\n    path = os.path.join(LOCAL_BASE_DIR, info['file'])\n    if os.path.exists(path):\n        img = Image.open(path).convert('RGB')\n        loaded_bases[key] = img\n        category = info.get('category', 'unknown')\n        print(f'  [{category}] {key}: {img.size[0]}x{img.size[1]}')\n    else:\n        print(f'  WARNING: Missing {info[\"file\"]}!')\n\nprint(f'\\nLoaded {len(loaded_bases)}/{len(needed_bases)} needed base images.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5. Manifest Statistics\n#@markdown Shows breakdown of this run's generation entries.\n\nfrom collections import Counter\n\nmode = f'Chunk {CHUNK_INDEX + 1}/{TOTAL_CHUNKS}' if TOTAL_CHUNKS > 1 else 'Single instance'\nprint(f'Mode: {mode}')\nprint(f'Images to generate: {len(MANIFEST)}')\nprint()\n\n# Per outfit\noutfit_counts = Counter(m['output_dir'] for m in MANIFEST)\nprint('By outfit:')\nfor outfit, count in outfit_counts.most_common():\n    print(f'  {outfit}: {count} images')\n\n# Per emotion\nprint()\nemotion_counts = Counter(m['emotion'] for m in MANIFEST)\nprint('By emotion:')\nfor emotion, count in emotion_counts.most_common():\n    print(f'  {emotion}: {count} images')\n\n# Per hairstyle\nprint()\nhair_counts = Counter(m['hairstyle'] for m in MANIFEST)\nprint('By hairstyle:')\nfor hair, count in hair_counts.most_common():\n    print(f'  {hair}: {count} images')\n\n# Estimate time\nEST_SECONDS_PER_IMAGE = 25  # Flux Kontext on T4 with FP8\ntotal_hours = (len(MANIFEST) * EST_SECONDS_PER_IMAGE) / 3600\nprint(f'\\nEstimated time: {total_hours:.1f} hours at ~{EST_SECONDS_PER_IMAGE}s/image')\nif TOTAL_CHUNKS <= 1:\n    print(f'({total_hours / 12:.0f} sessions at 12h each, resume support included)')\n    # Show dress priority\n    dress_count = sum(1 for m in MANIFEST if m['base_key'] == 'dress')\n    dress_hours = (dress_count * EST_SECONDS_PER_IMAGE) / 3600\n    print(f'\\ndress.jpg images: {dress_count} (first {dress_hours:.1f}h of generation)')\n    print('These generate FIRST - if the session times out, at least dress is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6. Batch Generation Engine\n#@markdown Generates all images using Flux Kontext image-to-image editing.\n#@markdown **Resume-safe:** skips images that already exist on Drive.\n\nimport time\nfrom pathlib import Path\n\n# Generation settings\nGUIDANCE_SCALE = 2.5          # Kontext recommended\nNUM_INFERENCE_STEPS = 28      # Quality/speed balance\nBASE_SEED = 42\n\n\ndef get_completed():\n    \"\"\"Find already-generated images for resume support.\"\"\"\n    done = set()\n    for d in Path(OUTPUT_ROOT).iterdir():\n        if d.is_dir() and not d.name.startswith('_'):\n            for f in d.glob('*.png'):\n                done.add(f'{d.name}/{f.name}')\n    return done\n\n\ndef generate_one(base_image, prompt, seed):\n    \"\"\"Generate a single avatar via Kontext image-to-image.\"\"\"\n    result = pipe(\n        image=base_image,\n        prompt=prompt,\n        guidance_scale=GUIDANCE_SCALE,\n        num_inference_steps=NUM_INFERENCE_STEPS,\n        generator=torch.Generator().manual_seed(seed),\n    )\n    return result.images[0]\n\n\ndef run_batch():\n    \"\"\"Run the full batch with resume support.\"\"\"\n    completed = get_completed()\n    total = len(MANIFEST)\n    generated = 0\n    skipped = 0\n    errors = []\n    all_meta = []\n\n    update_status(f'GENERATING 0/{total}')\n\n    for idx, entry in enumerate(MANIFEST, 1):\n        rel_path = f'{entry[\"output_dir\"]}/{entry[\"output_filename\"]}'\n\n        # Resume: skip existing\n        if rel_path in completed:\n            skipped += 1\n            all_meta.append({'path': rel_path, 'tags': entry['tags']})\n            if idx % 50 == 0:\n                print(f'[{idx}/{total}] Skipping existing...')\n            continue\n\n        # Get base image\n        base_key = entry['base_key']\n        if base_key not in loaded_bases:\n            errors.append(f'{rel_path}: missing base image {base_key}')\n            continue\n\n        base_img = loaded_bases[base_key]\n        seed = BASE_SEED + idx\n\n        # Ensure output dir\n        outfit_dir = os.path.join(OUTPUT_ROOT, entry['output_dir'])\n        os.makedirs(outfit_dir, exist_ok=True)\n\n        hairstyle = entry.get('hairstyle', 'original')\n        print(f'[{idx}/{total}] {rel_path} (base: {base_key}, hair: {hairstyle})')\n        print(f'  Prompt: {entry[\"prompt_text\"][:100]}...')\n\n        try:\n            t0 = time.time()\n            image = generate_one(base_img, entry['prompt_text'], seed)\n            elapsed = time.time() - t0\n\n            save_path = os.path.join(outfit_dir, entry['output_filename'])\n            image.save(save_path, 'PNG')\n\n            generated += 1\n            log_progress(entry['output_dir'], entry['output_filename'], idx, total)\n            all_meta.append({'path': rel_path, 'tags': entry['tags']})\n            print(f'  Done in {elapsed:.1f}s')\n\n            torch.cuda.empty_cache()\n\n        except Exception as e:\n            errors.append(f'{rel_path}: {e}')\n            print(f'  ERROR: {e}')\n\n        if idx % 10 == 0:\n            update_status(f'GENERATING {idx}/{total}')\n\n    # Write YAML config\n    cfg_path = os.path.join(OUTPUT_ROOT, '_image_config.yaml')\n    with open(cfg_path, 'w') as f:\n        f.write('# Generated avatar config - paste into pyagentvox.yaml\\n')\n        f.write('images:\\n')\n        for m in all_meta:\n            f.write(f'- path: {m[\"path\"]}\\n')\n            f.write('  tags:\\n')\n            for t in m['tags']:\n                f.write(f'  - {t}\\n')\n\n    if errors:\n        with open(os.path.join(OUTPUT_ROOT, '_errors.txt'), 'w') as f:\n            f.write('\\n'.join(errors))\n\n    print(f'\\n{\"=\" * 60}')\n    print(f'BATCH COMPLETE')\n    print(f'  Generated: {generated}')\n    print(f'  Skipped: {skipped}')\n    print(f'  Errors: {len(errors)}')\n    print(f'  Total: {total}')\n    print(f'{\"=\" * 60}')\n    update_status(f'COMPLETE {generated}/{total}')\n    return generated, skipped, errors\n\n\ngenerated, skipped, errors = run_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7. Preview Grid\n",
    "#@markdown Visual overview of all generated images.\n",
    "\n",
    "import math\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "def make_grid(root, thumb=128, cols=10):\n",
    "    imgs = []\n",
    "    for d in sorted(Path(root).iterdir()):\n",
    "        if d.is_dir() and not d.name.startswith('_'):\n",
    "            for f in sorted(d.glob('*.png')):\n",
    "                try:\n",
    "                    im = PILImage.open(f)\n",
    "                    im.thumbnail((thumb, thumb))\n",
    "                    imgs.append(im)\n",
    "                except Exception:\n",
    "                    pass\n",
    "    if not imgs:\n",
    "        print('No images to preview.')\n",
    "        return\n",
    "    rows = math.ceil(len(imgs) / cols)\n",
    "    grid = PILImage.new('RGB', (cols * thumb, rows * thumb), (30, 30, 30))\n",
    "    for i, im in enumerate(imgs):\n",
    "        r, c = divmod(i, cols)\n",
    "        x = c * thumb + (thumb - im.width) // 2\n",
    "        y = r * thumb + (thumb - im.height) // 2\n",
    "        grid.paste(im, (x, y))\n",
    "    grid.save(os.path.join(root, '_preview.png'))\n",
    "    print(f'{len(imgs)} images in {rows}x{cols} grid')\n",
    "    from IPython.display import display\n",
    "    display(grid)\n",
    "\n",
    "make_grid(OUTPUT_ROOT)\n",
    "update_status('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 8. Download ZIP\n",
    "#@markdown Creates a ZIP archive on Drive and offers browser download.\n",
    "\n",
    "import shutil\n",
    "\n",
    "zip_path = '/content/luna_avatars'\n",
    "shutil.make_archive(zip_path, 'zip', OUTPUT_ROOT)\n",
    "zip_file = f'{zip_path}.zip'\n",
    "size_mb = os.path.getsize(zip_file) / (1024 * 1024)\n",
    "print(f'ZIP: {zip_file} ({size_mb:.1f} MB)')\n",
    "\n",
    "shutil.copy2(zip_file, os.path.join(OUTPUT_ROOT, 'luna_avatars.zip'))\n",
    "print('Copied to Drive.')\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(zip_file)\n",
    "except Exception:\n",
    "    print('Download from Google Drive instead.')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "authorship_tag": "pyagentvox"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}