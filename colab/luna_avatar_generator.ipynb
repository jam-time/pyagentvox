{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Luna Avatar Generator - Flux Kontext Image-to-Image Pipeline\n\nGenerates **4,870** Luna avatar variations using **Flux.1 Kontext Dev** on a free Colab T4 GPU.\n\n**Prerequisites:**\n1. A [Hugging Face account](https://huggingface.co/join) with access to [FLUX.1-Kontext-dev](https://huggingface.co/black-forest-labs/FLUX.1-Kontext-dev) (accept the license on the model page)\n2. An HF access token stored in **Colab Secrets** as `HF_TOKEN` (Settings gear icon > Secrets > Add `HF_TOKEN`)\n3. A `BASE_IMAGES/` folder uploaded to your Google Drive root with 23 reference images\n4. `prompt_manifest.py` uploaded to your Drive root (or `Drive/colab/` folder)\n\n**How it works:** Takes 23 base reference images of Luna (9 regular outfits + 14 costumes),\nthen applies pose/emotion prompts via image-to-image editing. Regular outfit prompts are\nmultiplied by 4 hairstyle variants (hair down, messy bun, ponytail, braid). Costume prompts\nare thematic and not multiplied.\n\n**Priority order:** `dress.jpg` images generate first (396 images), then other outfits, then costumes.\n\n**Parallel mode:** Set `CHUNK_INDEX` and `TOTAL_CHUNKS` in cell 4 to split across multiple Colab instances.\n\n**Resume-safe** - skips images that already exist on Drive. Just re-run after timeout.\n\nResults are saved to Google Drive at `/MyDrive/luna_avatars/` with YAML config output."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 1. Setup Environment & Install Dependencies\n#@markdown Installs Flux Kontext pipeline and GGUF loading support. ~3-4 min on first run.\n#@markdown\n#@markdown **IMPORTANT:** You must have a Hugging Face account and accept the\n#@markdown [FLUX.1-Kontext-dev license](https://huggingface.co/black-forest-labs/FLUX.1-Kontext-dev)\n#@markdown before running this notebook. Store your HF token in Colab Secrets as `HF_TOKEN`.\n\nimport subprocess\nimport sys\nimport os\n\n# Install diffusers from main (Kontext support) + GGUF loading deps.\n# gguf: required for GGUFQuantizationConfig / from_single_file GGUF loading in diffusers.\n# accelerate: required for device_map / cpu_offload support.\n# optimum-quanto and bitsandbytes are NOT needed with the GGUF approach.\nsubprocess.check_call([\n    sys.executable, '-m', 'pip', 'install', '-q',\n    'git+https://github.com/huggingface/diffusers.git',\n    'transformers', 'accelerate', 'sentencepiece', 'protobuf',\n    'safetensors', 'Pillow', 'pyyaml', 'huggingface_hub',\n    'gguf',\n])\n\n# Authenticate with Hugging Face (FLUX.1-Kontext-dev is a gated model)\nfrom huggingface_hub import login\ntry:\n    from google.colab import userdata\n    hf_token = userdata.get('HF_TOKEN')\n    login(token=hf_token)\n    print('Authenticated with HF token from Colab Secrets.')\nexcept Exception:\n    # Fall back to interactive login if secret not found\n    print('HF_TOKEN not found in Colab Secrets. Trying interactive login...')\n    login()\n\n# Verify GPU\nimport torch\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n    print(f'GPU: {gpu_name} ({gpu_mem:.1f} GB)')\n    print(f'CUDA: {torch.version.cuda}')\n    print(f'PyTorch: {torch.__version__}')\nelse:\n    raise RuntimeError(\n        'No GPU detected! Go to Runtime > Change runtime type > T4 GPU'\n    )\n\nprint('\\nEnvironment ready.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 2. Mount Google Drive & Upload Base Images\n#@markdown Mounts Drive and copies base images to the working directory.\n#@markdown Also upload `prompt_manifest.py` to your Drive root (or the `colab/` folder).\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nOUTPUT_ROOT = '/content/drive/MyDrive/luna_avatars'\nLOCAL_BASE_DIR = '/content/base_images'\n\nos.makedirs(OUTPUT_ROOT, exist_ok=True)\nos.makedirs(LOCAL_BASE_DIR, exist_ok=True)\n\n# Search for BASE_IMAGES folder in common Drive locations\nBASE_IMAGE_SEARCH = [\n    '/content/drive/MyDrive/BASE_IMAGES',\n    '/content/drive/MyDrive/Luna/BASE_IMAGES',\n    '/content/drive/MyDrive/luna/BASE_IMAGES',\n    '/content/drive/MyDrive/colab/BASE_IMAGES',\n]\nBASE_IMAGE_DIR = None\nfor candidate in BASE_IMAGE_SEARCH:\n    if os.path.isdir(candidate):\n        BASE_IMAGE_DIR = candidate\n        print(f'Found base images at: {candidate}')\n        break\n\n# Copy base images to local storage for faster access (supports jpg, png, webp)\nimport shutil\nif BASE_IMAGE_DIR:\n    for f in os.listdir(BASE_IMAGE_DIR):\n        if f.lower().endswith(('.jpg', '.jpeg', '.png', '.webp')):\n            src = os.path.join(BASE_IMAGE_DIR, f)\n            dst = os.path.join(LOCAL_BASE_DIR, f)\n            shutil.copy2(src, dst)\n            print(f'  Copied: {f}')\n    print(f'\\n{len(os.listdir(LOCAL_BASE_DIR))} base images ready.')\nelse:\n    print('WARNING: BASE_IMAGES folder not found on Drive!')\n    print('Searched:', BASE_IMAGE_SEARCH)\n    print('Expected: 9 regular outfit images + 14 costume images (23 total)')\n\n# Copy prompt_manifest.py to working directory for import\nMANIFEST_LOCATIONS = [\n    '/content/drive/MyDrive/prompt_manifest.py',\n    '/content/drive/MyDrive/colab/prompt_manifest.py',\n    '/content/drive/MyDrive/Luna/prompt_manifest.py',\n    '/content/drive/MyDrive/Luna/colab/prompt_manifest.py',\n]\nmanifest_found = False\nfor loc in MANIFEST_LOCATIONS:\n    if os.path.exists(loc):\n        shutil.copy2(loc, '/content/prompt_manifest.py')\n        print(f'\\nCopied prompt_manifest.py from {loc}')\n        manifest_found = True\n        break\nif not manifest_found:\n    print('\\nWARNING: prompt_manifest.py not found on Drive!')\n    print('Upload it to your Drive root or Drive/colab/ folder.')\n\n# Progress tracking\nPROGRESS_FILE = os.path.join(OUTPUT_ROOT, '_progress.txt')\nSTATUS_FILE = os.path.join(OUTPUT_ROOT, '_status.txt')\n\ndef update_status(msg):\n    with open(STATUS_FILE, 'w') as f:\n        f.write(msg)\n    print(msg)\n\ndef log_progress(outfit, filename, idx, total):\n    with open(PROGRESS_FILE, 'a') as f:\n        f.write(f'{idx}/{total} | {outfit}/{filename}\\n')\n\nupdate_status('MOUNTED')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 3. Load Flux.1 Kontext Dev (GGUF pre-quantized, T4-safe)\n#@markdown Loads the transformer from a pre-quantized GGUF file so that RAM never holds\n#@markdown the full bfloat16 weights at any point. Same for T5. Both stay on CPU until\n#@markdown `enable_model_cpu_offload()` shuttles them to VRAM one at a time during inference.\n#@markdown\n#@markdown **Why GGUF over optimum-quanto FP8 (previous approach):**\n#@markdown\n#@markdown The optimum-quanto FP8 strategy loads the full bfloat16 transformer to CPU RAM\n#@markdown first (~12 GB), then quantizes in-place. On a T4 with 12.7 GB system RAM that\n#@markdown barely fits — and leaves no headroom for T5. Any shard buffering during download\n#@markdown can push it over the edge, causing a RAM OOM before quantization even starts.\n#@markdown\n#@markdown **GGUF solution (this cell):**\n#@markdown The quantized weights live in the `.gguf` file on disk. `from_single_file()` with\n#@markdown `GGUFQuantizationConfig` reads them directly in their quantized form — the full\n#@markdown bfloat16 model is never materialized in RAM at all. Peak RAM during loading:\n#@markdown - Transformer Q4_K_M (6.93 GB on disk) → ~7 GB RAM peak\n#@markdown - T5 Q5_K_M (3.39 GB on disk) → ~3.5 GB RAM peak\n#@markdown - Total peak (after transformer is fully loaded and T5 begins): ~7 + 3.5 = ~10.5 GB\n#@markdown - Well within the T4's 12.7 GB system RAM.\n#@markdown\n#@markdown **VRAM usage at load time:** 0 GB (both models land on CPU, not CUDA).\n#@markdown `enable_model_cpu_offload()` then moves each sub-model to VRAM only during its\n#@markdown forward pass and returns it to CPU immediately after — peak inference VRAM ~12-13 GB.\n#@markdown\n#@markdown **Quality note:** Q4_K_M transformer is virtually indistinguishable from bfloat16\n#@markdown for image generation tasks. Q5_K_M T5 text encoder is recommended by city96 as\n#@markdown the minimum quality threshold for T5 GGUF (larger is better, but Q5_K_M fits here).\n#@markdown\n#@markdown **Sources:**\n#@markdown - GGUF for Kontext confirmed working: https://github.com/huggingface/diffusers/issues/11962\n#@markdown - Transformer files: https://huggingface.co/QuantStack/FLUX.1-Kontext-dev-GGUF\n#@markdown - T5 GGUF files: https://huggingface.co/city96/t5-v1_1-xxl-encoder-gguf\n\nimport gc\nimport torch\nfrom diffusers import FluxKontextPipeline, FluxTransformer2DModel, GGUFQuantizationConfig\nfrom transformers import T5EncoderModel\n\nupdate_status('LOADING_MODEL')\n\nMODEL_ID = 'black-forest-labs/FLUX.1-Kontext-dev'\n\n# ── Transformer (GGUF Q4_K_M, 6.93 GB on disk) ────────────────────────────────\n# from_single_file() with GGUFQuantizationConfig reads the quantized weights\n# directly from the GGUF file. The full bfloat16 model is NEVER materialized in RAM.\n# Weights remain as torch.uint8 tensors and are dequantized to bfloat16 on-the-fly\n# during each forward pass (dynamic dequantization, no CUDA needed at load time).\n# config= + subfolder= tell diffusers where to find the architecture config JSON\n# since a single GGUF file does not embed the full diffusers config dict.\nprint('Loading transformer from GGUF Q4_K_M (6.93 GB, quantized weights only)...')\ntransformer = FluxTransformer2DModel.from_single_file(\n    'https://huggingface.co/QuantStack/FLUX.1-Kontext-dev-GGUF/blob/main/flux1-kontext-dev-Q4_K_M.gguf',\n    quantization_config=GGUFQuantizationConfig(compute_dtype=torch.bfloat16),\n    torch_dtype=torch.bfloat16,\n    config=MODEL_ID,\n    subfolder='transformer',\n)\n\ngc.collect()\nvram_mb = torch.cuda.memory_allocated() / 1e6\nprint(f'  Transformer ready. VRAM used: {vram_mb:.0f} MB (expected: ~0)')\n\n# ── T5 Text Encoder (GGUF Q5_K_M, 3.39 GB on disk) ───────────────────────────\n# city96's t5-v1_1-xxl-encoder-gguf provides the T5 encoder quantized to GGUF.\n# Loaded via transformers from_pretrained() with gguf_file= parameter, which\n# reads quantized weights directly from disk — again, no full bfloat16 copy in RAM.\n# Q5_K_M is the minimum recommended quality level for T5 GGUF (per city96).\n# At this point transformer is ~7 GB of quantized tensors in RAM; T5 adds ~3.5 GB\n# for a total peak of ~10.5 GB — within the T4's 12.7 GB system RAM.\nprint('Loading T5 text encoder from GGUF Q5_K_M (3.39 GB, quantized weights only)...')\ntext_encoder_2 = T5EncoderModel.from_pretrained(\n    'city96/t5-v1_1-xxl-encoder-gguf',\n    gguf_file='t5-v1_1-xxl-encoder-Q5_K_M.gguf',\n    torch_dtype=torch.bfloat16,\n)\n\ngc.collect()\nvram_mb = torch.cuda.memory_allocated() / 1e6\nprint(f'  T5 ready. VRAM used: {vram_mb:.0f} MB (expected: ~0)')\n\n# ── Assemble pipeline ──────────────────────────────────────────────────────────\n# Pass the pre-loaded GGUF components. VAE, CLIP text encoder, and scheduler are\n# small (~300 MB combined) and load normally from the gated HF repo.\n# low_cpu_mem_usage prevents an extra full-precision copy during pipeline assembly.\nprint('Building pipeline from pre-loaded GGUF components...')\npipe = FluxKontextPipeline.from_pretrained(\n    MODEL_ID,\n    transformer=transformer,\n    text_encoder_2=text_encoder_2,\n    torch_dtype=torch.bfloat16,\n    low_cpu_mem_usage=True,\n)\n\n# CPU offload: each sub-model moves to CUDA only during its forward pass, then\n# immediately returns to CPU. This keeps peak VRAM at ~12-13 GB (within T4's 15 GB)\n# rather than trying to hold all components on GPU simultaneously (~20+ GB).\npipe.enable_model_cpu_offload()\n\ngc.collect()\ntorch.cuda.empty_cache()\n\nvram_alloc = torch.cuda.memory_allocated() / 1e9\nvram_reserved = torch.cuda.memory_reserved() / 1e9\nprint(f'\\nPipeline ready.')\nprint(f'  VRAM allocated : {vram_alloc:.2f} GB')\nprint(f'  VRAM reserved  : {vram_reserved:.2f} GB')\nupdate_status('MODEL_READY')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4. Load Base Images & Import Manifest\n#@markdown Loads all 23 base reference images and imports the prompt manifest.\n#@markdown\n#@markdown **Parallel mode:** To split across multiple Colab instances, set\n#@markdown `TOTAL_CHUNKS` to the number of instances and `CHUNK_INDEX` to this\n#@markdown instance's index (0-based). Each instance gets a different slice.\n#@markdown Leave both at 0 for single-instance mode (generates everything).\n\nfrom PIL import Image\nimport sys\n\n# â”€â”€ PARALLEL CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Set these to split work across multiple Colab instances:\n#   CHUNK_INDEX = 0, TOTAL_CHUNKS = 0  -> single instance, all 4870 images\n#   CHUNK_INDEX = 0, TOTAL_CHUNKS = 3  -> instance 1 of 3 (~1600 images)\n#   CHUNK_INDEX = 1, TOTAL_CHUNKS = 3  -> instance 2 of 3 (~1600 images)\n#   CHUNK_INDEX = 2, TOTAL_CHUNKS = 3  -> instance 3 of 3 (~1600 images)\nCHUNK_INDEX = 0   #@param {type:\"integer\"}\nTOTAL_CHUNKS = 0  #@param {type:\"integer\"}\n\n# Import prompt_manifest (copied to /content/ in cell 2)\nsys.path.insert(0, '/content')\nfrom prompt_manifest import BASE_IMAGES, MANIFEST as FULL_MANIFEST, MASTER_PROMPTS, HAIRSTYLE_VARIANTS, get_chunk\n\n# Apply chunking if configured\nif TOTAL_CHUNKS > 1:\n    MANIFEST = get_chunk(CHUNK_INDEX, TOTAL_CHUNKS)\n    print(f'PARALLEL MODE: Chunk {CHUNK_INDEX + 1} of {TOTAL_CHUNKS}')\n    print(f'This instance: {len(MANIFEST)} images (of {len(FULL_MANIFEST)} total)')\nelse:\n    MANIFEST = FULL_MANIFEST\n    print(f'SINGLE MODE: All {len(MANIFEST)} images')\n\n# Show priority: first base_key in this chunk's manifest\nfrom collections import Counter\nbase_order = []\nfor m in MANIFEST:\n    if m['base_key'] not in base_order:\n        base_order.append(m['base_key'])\nbase_counts = Counter(m['base_key'] for m in MANIFEST)\nprint(f'\\nGeneration order for this {\"chunk\" if TOTAL_CHUNKS > 1 else \"run\"}:')\ncumulative = 0\nfor key in base_order[:5]:\n    count = base_counts[key]\n    cumulative += count\n    print(f'  {key}: {count} images (cumulative: {cumulative})')\nif len(base_order) > 5:\n    print(f'  ... and {len(base_order) - 5} more base images')\n\n# Load only the base images needed for this chunk\nneeded_bases = set(m['base_key'] for m in MANIFEST)\nloaded_bases = {}\nfor key, info in BASE_IMAGES.items():\n    if key not in needed_bases:\n        continue\n    path = os.path.join(LOCAL_BASE_DIR, info['file'])\n    if os.path.exists(path):\n        img = Image.open(path).convert('RGB')\n        loaded_bases[key] = img\n        category = info.get('category', 'unknown')\n        print(f'  [{category}] {key}: {img.size[0]}x{img.size[1]}')\n    else:\n        print(f'  WARNING: Missing {info[\"file\"]}!')\n\nprint(f'\\nLoaded {len(loaded_bases)}/{len(needed_bases)} needed base images.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5. Manifest Statistics\n#@markdown Shows breakdown of this run's generation entries.\n\nfrom collections import Counter\n\nmode = f'Chunk {CHUNK_INDEX + 1}/{TOTAL_CHUNKS}' if TOTAL_CHUNKS > 1 else 'Single instance'\nprint(f'Mode: {mode}')\nprint(f'Images to generate: {len(MANIFEST)}')\nprint()\n\n# Per outfit\noutfit_counts = Counter(m['output_dir'] for m in MANIFEST)\nprint('By outfit:')\nfor outfit, count in outfit_counts.most_common():\n    print(f'  {outfit}: {count} images')\n\n# Per emotion\nprint()\nemotion_counts = Counter(m['emotion'] for m in MANIFEST)\nprint('By emotion:')\nfor emotion, count in emotion_counts.most_common():\n    print(f'  {emotion}: {count} images')\n\n# Per hairstyle\nprint()\nhair_counts = Counter(m['hairstyle'] for m in MANIFEST)\nprint('By hairstyle:')\nfor hair, count in hair_counts.most_common():\n    print(f'  {hair}: {count} images')\n\n# Estimate time\nEST_SECONDS_PER_IMAGE = 25  # Flux Kontext on T4 with FP8\ntotal_hours = (len(MANIFEST) * EST_SECONDS_PER_IMAGE) / 3600\nprint(f'\\nEstimated time: {total_hours:.1f} hours at ~{EST_SECONDS_PER_IMAGE}s/image')\nif TOTAL_CHUNKS <= 1:\n    print(f'({total_hours / 12:.0f} sessions at 12h each, resume support included)')\n    # Show dress priority\n    dress_count = sum(1 for m in MANIFEST if m['base_key'] == 'dress')\n    dress_hours = (dress_count * EST_SECONDS_PER_IMAGE) / 3600\n    print(f'\\ndress.jpg images: {dress_count} (first {dress_hours:.1f}h of generation)')\n    print('These generate FIRST - if the session times out, at least dress is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6. Batch Generation Engine\n#@markdown Generates all images using Flux Kontext image-to-image editing.\n#@markdown **Resume-safe:** skips images that already exist on Drive.\n\nimport time\nfrom pathlib import Path\n\n# Generation settings\nGUIDANCE_SCALE = 2.5          # Kontext recommended\nNUM_INFERENCE_STEPS = 28      # Quality/speed balance\nBASE_SEED = 42\n\n\ndef get_completed():\n    \"\"\"Find already-generated images for resume support.\"\"\"\n    done = set()\n    for d in Path(OUTPUT_ROOT).iterdir():\n        if d.is_dir() and not d.name.startswith('_'):\n            for f in d.glob('*.png'):\n                done.add(f'{d.name}/{f.name}')\n    return done\n\n\ndef generate_one(base_image, prompt, seed):\n    \"\"\"Generate a single avatar via Kontext image-to-image.\"\"\"\n    result = pipe(\n        image=base_image,\n        prompt=prompt,\n        guidance_scale=GUIDANCE_SCALE,\n        num_inference_steps=NUM_INFERENCE_STEPS,\n        generator=torch.Generator().manual_seed(seed),\n    )\n    return result.images[0]\n\n\ndef run_batch():\n    \"\"\"Run the full batch with resume support.\"\"\"\n    completed = get_completed()\n    total = len(MANIFEST)\n    generated = 0\n    skipped = 0\n    errors = []\n    all_meta = []\n\n    update_status(f'GENERATING 0/{total}')\n\n    for idx, entry in enumerate(MANIFEST, 1):\n        rel_path = f'{entry[\"output_dir\"]}/{entry[\"output_filename\"]}'\n\n        # Resume: skip existing\n        if rel_path in completed:\n            skipped += 1\n            all_meta.append({'path': rel_path, 'tags': entry['tags']})\n            if idx % 50 == 0:\n                print(f'[{idx}/{total}] Skipping existing...')\n            continue\n\n        # Get base image\n        base_key = entry['base_key']\n        if base_key not in loaded_bases:\n            errors.append(f'{rel_path}: missing base image {base_key}')\n            continue\n\n        base_img = loaded_bases[base_key]\n        seed = BASE_SEED + idx\n\n        # Ensure output dir\n        outfit_dir = os.path.join(OUTPUT_ROOT, entry['output_dir'])\n        os.makedirs(outfit_dir, exist_ok=True)\n\n        hairstyle = entry.get('hairstyle', 'original')\n        print(f'[{idx}/{total}] {rel_path} (base: {base_key}, hair: {hairstyle})')\n        print(f'  Prompt: {entry[\"prompt_text\"][:100]}...')\n\n        try:\n            t0 = time.time()\n            image = generate_one(base_img, entry['prompt_text'], seed)\n            elapsed = time.time() - t0\n\n            save_path = os.path.join(outfit_dir, entry['output_filename'])\n            image.save(save_path, 'PNG')\n\n            generated += 1\n            log_progress(entry['output_dir'], entry['output_filename'], idx, total)\n            all_meta.append({'path': rel_path, 'tags': entry['tags']})\n            print(f'  Done in {elapsed:.1f}s')\n\n            torch.cuda.empty_cache()\n\n        except Exception as e:\n            errors.append(f'{rel_path}: {e}')\n            print(f'  ERROR: {e}')\n\n        if idx % 10 == 0:\n            update_status(f'GENERATING {idx}/{total}')\n\n    # Write YAML config\n    cfg_path = os.path.join(OUTPUT_ROOT, '_image_config.yaml')\n    with open(cfg_path, 'w') as f:\n        f.write('# Generated avatar config - paste into pyagentvox.yaml\\n')\n        f.write('images:\\n')\n        for m in all_meta:\n            f.write(f'- path: {m[\"path\"]}\\n')\n            f.write('  tags:\\n')\n            for t in m['tags']:\n                f.write(f'  - {t}\\n')\n\n    if errors:\n        with open(os.path.join(OUTPUT_ROOT, '_errors.txt'), 'w') as f:\n            f.write('\\n'.join(errors))\n\n    print(f'\\n{\"=\" * 60}')\n    print(f'BATCH COMPLETE')\n    print(f'  Generated: {generated}')\n    print(f'  Skipped: {skipped}')\n    print(f'  Errors: {len(errors)}')\n    print(f'  Total: {total}')\n    print(f'{\"=\" * 60}')\n    update_status(f'COMPLETE {generated}/{total}')\n    return generated, skipped, errors\n\n\ngenerated, skipped, errors = run_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7. Preview Grid\n",
    "#@markdown Visual overview of all generated images.\n",
    "\n",
    "import math\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "def make_grid(root, thumb=128, cols=10):\n",
    "    imgs = []\n",
    "    for d in sorted(Path(root).iterdir()):\n",
    "        if d.is_dir() and not d.name.startswith('_'):\n",
    "            for f in sorted(d.glob('*.png')):\n",
    "                try:\n",
    "                    im = PILImage.open(f)\n",
    "                    im.thumbnail((thumb, thumb))\n",
    "                    imgs.append(im)\n",
    "                except Exception:\n",
    "                    pass\n",
    "    if not imgs:\n",
    "        print('No images to preview.')\n",
    "        return\n",
    "    rows = math.ceil(len(imgs) / cols)\n",
    "    grid = PILImage.new('RGB', (cols * thumb, rows * thumb), (30, 30, 30))\n",
    "    for i, im in enumerate(imgs):\n",
    "        r, c = divmod(i, cols)\n",
    "        x = c * thumb + (thumb - im.width) // 2\n",
    "        y = r * thumb + (thumb - im.height) // 2\n",
    "        grid.paste(im, (x, y))\n",
    "    grid.save(os.path.join(root, '_preview.png'))\n",
    "    print(f'{len(imgs)} images in {rows}x{cols} grid')\n",
    "    from IPython.display import display\n",
    "    display(grid)\n",
    "\n",
    "make_grid(OUTPUT_ROOT)\n",
    "update_status('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 8. Download ZIP\n",
    "#@markdown Creates a ZIP archive on Drive and offers browser download.\n",
    "\n",
    "import shutil\n",
    "\n",
    "zip_path = '/content/luna_avatars'\n",
    "shutil.make_archive(zip_path, 'zip', OUTPUT_ROOT)\n",
    "zip_file = f'{zip_path}.zip'\n",
    "size_mb = os.path.getsize(zip_file) / (1024 * 1024)\n",
    "print(f'ZIP: {zip_file} ({size_mb:.1f} MB)')\n",
    "\n",
    "shutil.copy2(zip_file, os.path.join(OUTPUT_ROOT, 'luna_avatars.zip'))\n",
    "print('Copied to Drive.')\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(zip_file)\n",
    "except Exception:\n",
    "    print('Download from Google Drive instead.')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "authorship_tag": "pyagentvox"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}